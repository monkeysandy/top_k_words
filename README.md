# Top K words in a big dataset
In this assignment we are going to find the top k frequent word in the big dataset, which is 300MB, 2.5GB and 16GB. We present an analysis of the performance of our implementation of a top-K most frequent words algorithm using different input dataset sizes and optimization techniques. We evaluate the algorithm's efficiency in terms of execution time, memory usage, and CPU utilization, and we analyze the impact of different algorithms, data structures, and system resources on its performance. 

# How to use it
1. Run "topkwords.py"

2. input the filepath and stopword path

3. choose how many words you would like to see (k)

4. set the sharding number

